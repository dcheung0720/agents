import { LLM, BaseLLMParams } from '@langchain/core/language_models/llms'
import { getEnvironmentVariable } from '../../../src/utils'
import { GenerationChunk } from '@langchain/core/outputs'
import { CallbackManagerForLLMRun } from '@langchain/core/callbacks/manager'

export interface CustomLLMInput {
    apiKey?: string
    endpointUrl: string
}

export class CustomAPIInference extends LLM implements CustomLLMInput {
    get lc_secrets(): { [key: string]: string } | undefined {
        return {
            apiKey: 'CUSTOM_API_KEY'
        }
    }

    apiKey: string
    endpointUrl: string

    constructor(fields: CustomLLMInput & BaseLLMParams) {
        super(fields)

        this.apiKey = fields.apiKey ?? getEnvironmentVariable('CUSTOM_API_KEY')
        this.endpointUrl = fields.endpointUrl

        if (!this.apiKey) {
            throw new Error(
                'Please set an API key for the custom API in the environment variable CUSTOM_API_KEY or in the apiKey field of the CustomAPIInference constructor.'
            )
        }

        if (!this.endpointUrl) {
            throw new Error('Custom LLM endpointUrl must be provided.')
        }
    }

    _llmType() {
        return 'custom_api'
    }

    invocationParams(prompt: string) {
        return {
            prompt
        }
    }

    async _call(prompt: string, options: this['ParsedCallOptions']): Promise<string> {
        const payload = this.invocationParams(prompt)

        const headers = {
            'Content-Type': 'application/json',
            Authorization: `Bearer ${this.apiKey}`
        }

        const res = await this.caller.callWithOptions({ signal: options.signal }, async () => {
            const response = await fetch(this.endpointUrl, {
                method: 'POST',
                headers,
                body: JSON.stringify(payload)
            })

            if (!response.ok) {
                const errorText = await response.text()
                throw new Error(`Custom API error: ${response.status} ${errorText}`)
            }

            const data = await response.json()
            return data.generated_text || ''
        })

        return res
    }

    async *_streamResponseChunks(
        _prompt: string,
        _options: this['ParsedCallOptions'],
        _runManager?: CallbackManagerForLLMRun
    ): AsyncGenerator<GenerationChunk> {
        throw new Error('Streaming not supported for CustomAPIInference')
    }
}
